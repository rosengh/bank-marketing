{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Bank Marketing Project***\n",
    "## Nguyen Hoang Phuong Anh \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome Report \n",
    "\n",
    "\n",
    "First of all, below are all of the library used. This project is mainly run on PySpark and its extensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, SparseVector\n",
    "from pyspark.sql.types import IntegerType,StringType,DoubleType\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: DataFrame\n",
    "1. Read data in bank-marketing.csv in a DataFrame, run and show the first 20 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/21 01:37:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/21 01:37:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df0 schema is: \n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- dayOfWeek: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- empVarRate: double (nullable = true)\n",
      " |-- consPriceIdx: double (nullable = true)\n",
      " |-- consConfIdx: double (nullable = true)\n",
      " |-- euribor3m: double (nullable = true)\n",
      " |-- nrEmployed: double (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n",
      "+---+-----------+--------+-------------------+-------+-------+----+---------+-----+---------+--------+--------+-----+--------+-----------+----------+------------+-----------+---------+----------+---+\n",
      "|age|        job| marital|          education|default|housing|loan|  contact|month|dayOfWeek|duration|campaign|pdays|previous|   poutcome|empVarRate|consPriceIdx|consConfIdx|euribor3m|nrEmployed|  y|\n",
      "+---+-----------+--------+-------------------+-------+-------+----+---------+-----+---------+--------+--------+-----+--------+-----------+----------+------------+-----------+---------+----------+---+\n",
      "| 56|  housemaid| married|           basic.4y|     no|     no|  no|telephone|  may|      mon|     261|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 57|   services| married|        high.school|unknown|     no|  no|telephone|  may|      mon|     149|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 37|   services| married|        high.school|     no|    yes|  no|telephone|  may|      mon|     226|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 40|     admin.| married|           basic.6y|     no|     no|  no|telephone|  may|      mon|     151|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 56|   services| married|        high.school|     no|     no| yes|telephone|  may|      mon|     307|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 45|   services| married|           basic.9y|unknown|     no|  no|telephone|  may|      mon|     198|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 59|     admin.| married|professional.course|     no|     no|  no|telephone|  may|      mon|     139|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 41|blue-collar| married|            unknown|unknown|     no|  no|telephone|  may|      mon|     217|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 24| technician|  single|professional.course|     no|    yes|  no|telephone|  may|      mon|     380|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 25|   services|  single|        high.school|     no|    yes|  no|telephone|  may|      mon|      50|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 41|blue-collar| married|            unknown|unknown|     no|  no|telephone|  may|      mon|      55|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 25|   services|  single|        high.school|     no|    yes|  no|telephone|  may|      mon|     222|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 29|blue-collar|  single|        high.school|     no|     no| yes|telephone|  may|      mon|     137|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 57|  housemaid|divorced|           basic.4y|     no|    yes|  no|telephone|  may|      mon|     293|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 35|blue-collar| married|           basic.6y|     no|    yes|  no|telephone|  may|      mon|     146|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 54|    retired| married|           basic.9y|unknown|    yes| yes|telephone|  may|      mon|     174|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 35|blue-collar| married|           basic.6y|     no|    yes|  no|telephone|  may|      mon|     312|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 46|blue-collar| married|           basic.6y|unknown|    yes| yes|telephone|  may|      mon|     440|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 50|blue-collar| married|           basic.9y|     no|    yes| yes|telephone|  may|      mon|     353|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "| 39| management|  single|           basic.9y|unknown|     no|  no|telephone|  may|      mon|     195|       1|  999|       0|nonexistent|       1.1|      93.994|      -36.4|    4.857|    5191.0| no|\n",
      "+---+-----------+--------+-------------------+-------+-------+----+---------+-----+---------+--------+--------+-----+--------+-----------+----------+------------+-----------+---------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#DataFrame (use basic pyspark.sql)\n",
    "# 1. Load the \"Bank-marketing\" dataset into a DataFrame, check the schema\n",
    "# and dislay the first 20 rows\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .appName(\"Bank Marketing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df0 = spark.read.csv(\"/Users/hoangphuonganhnguyen/Downloads/BankMarketing.csv\",header=True,inferSchema=True)\n",
    "\n",
    "print(\"df0 schema is: \")\n",
    "df0.printSchema()\n",
    "\n",
    "df0.show(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|          education|count|\n",
      "+-------------------+-----+\n",
      "|        high.school| 9515|\n",
      "|            unknown| 1731|\n",
      "|           basic.6y| 2292|\n",
      "|professional.course| 5243|\n",
      "|  university.degree|12168|\n",
      "|         illiterate|   18|\n",
      "|           basic.4y| 4176|\n",
      "|           basic.9y| 6045|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. groupBy education\n",
    "education = df0.groupBy(\"education\").count()\n",
    "education.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 31| 1947|\n",
      "| 32| 1846|\n",
      "| 33| 1833|\n",
      "| 36| 1780|\n",
      "| 35| 1759|\n",
      "| 34| 1745|\n",
      "| 30| 1714|\n",
      "| 37| 1475|\n",
      "| 29| 1453|\n",
      "| 39| 1432|\n",
      "| 38| 1407|\n",
      "| 41| 1278|\n",
      "| 40| 1161|\n",
      "| 42| 1142|\n",
      "| 45| 1103|\n",
      "| 43| 1055|\n",
      "| 46| 1030|\n",
      "| 44| 1011|\n",
      "| 28| 1001|\n",
      "| 48|  979|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. col \"age\", most targeted range\n",
    "age = df0.groupBy(\"age\").count()\n",
    "age_sort = age.orderBy(\"count\", ascending=False)\n",
    "age_sort.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|          education|count|\n",
      "+-------------------+-----+\n",
      "|        high.school| 9515|\n",
      "|            unknown| 1731|\n",
      "|           basic.6y| 2292|\n",
      "|professional.course| 5243|\n",
      "|  university.degree|12168|\n",
      "|         illiterate|   18|\n",
      "|           basic.4y| 4176|\n",
      "|           basic.9y| 6045|\n",
      "+-------------------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "| marital|count|\n",
      "+--------+-----+\n",
      "| unknown|   80|\n",
      "|divorced| 4612|\n",
      "| married|24928|\n",
      "|  single|11568|\n",
      "+--------+-----+\n",
      "\n",
      "+-------+------------------+-----------------+\n",
      "|summary|               age|         duration|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|             41188|            41188|\n",
      "|   mean| 40.02406040594348|258.2850101971448|\n",
      "| stddev|10.421249980934057| 259.279248836465|\n",
      "|    min|                17|                0|\n",
      "|    max|                98|             4918|\n",
      "+-------+------------------+-----------------+\n",
      "\n",
      "+--------+-----+\n",
      "| marital|count|\n",
      "+--------+-----+\n",
      "| unknown|   80|\n",
      "|divorced| 4612|\n",
      "| married|24928|\n",
      "|  single|11568|\n",
      "+--------+-----+\n",
      "\n",
      "+-------------------+-----+\n",
      "|          education|count|\n",
      "+-------------------+-----+\n",
      "|        high.school| 9515|\n",
      "|            unknown| 1731|\n",
      "|           basic.6y| 2292|\n",
      "|professional.course| 5243|\n",
      "|  university.degree|12168|\n",
      "|         illiterate|   18|\n",
      "|           basic.4y| 4176|\n",
      "|           basic.9y| 6045|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4 model report of selected cols\n",
    "selected_cols = df0.select(\"age\",'education','marital',\"duration\")\n",
    "\n",
    "selected_cols.groupBy(\"education\").count().show()\n",
    "selected_cols.groupBy(\"marital\").count().show()\n",
    "\n",
    "numerical_info = selected_cols.describe(\"age\",\"duration\")\n",
    "numerical_info.show()\n",
    "\n",
    "marital_info = selected_cols.groupBy(\"marital\").count().show()\n",
    "education_info = selected_cols.groupBy(\"education\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: \n",
      "['age', 'duration', 'campaign', 'pdays', 'previous', 'empVarRate', 'consPriceIdx', 'consConfIdx', 'euribor3m', 'nrEmployed']\n",
      "Categorical columns: \n",
      "['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'dayOfWeek', 'poutcome', 'y']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType,StringType,DoubleType\n",
    "#5. automatically identify the features and filter them based on their types (numerical or categorical)\n",
    "numerical_col = [field.name for field in df0.schema.fields\n",
    "                 if isinstance(field.dataType, (IntegerType, DoubleType))]\n",
    "categorical_col = [field.name for field in df0.schema.fields\n",
    "                   if isinstance(field.dataType,(StringType))]\n",
    "print(\"Numerical columns: \")\n",
    "print(numerical_col)\n",
    "print(\"Categorical columns: \")\n",
    "print(categorical_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 marital index schema is: \n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- dayOfWeek: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- empVarRate: double (nullable = true)\n",
      " |-- consPriceIdx: double (nullable = true)\n",
      " |-- consConfIdx: double (nullable = true)\n",
      " |-- euribor3m: double (nullable = true)\n",
      " |-- nrEmployed: double (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      " |-- maritalIndex: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "#Feature transformation (use pyspark.ml.feature)\n",
    "marital_Index = StringIndexer(inputCol=\"marital\", outputCol=\"maritalIndex\")\n",
    "df1 = marital_Index.fit(df0).transform(df0)\n",
    "print(\"df1 marital index schema is: \")\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 marital index schema is: \n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- dayOfWeek: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- empVarRate: double (nullable = true)\n",
      " |-- consPriceIdx: double (nullable = true)\n",
      " |-- consConfIdx: double (nullable = true)\n",
      " |-- euribor3m: double (nullable = true)\n",
      " |-- nrEmployed: double (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      " |-- maritalIndex: double (nullable = false)\n",
      " |-- maritalVector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marital_vector = OneHotEncoder(inputCol=\"maritalIndex\", outputCol=\"maritalVector\")\n",
    "df2 = marital_vector.fit(df1).transform(df1) #no maritalIndex col\n",
    "print(\"df2 marital index schema is: \")\n",
    "df2.printSchema()\n",
    "#define function to measure the vector size (features' sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_size_udf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df2\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaritalSize\u001b[39m\u001b[38;5;124m\"\u001b[39m,vector_size_udf(df2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaritalVector\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectors sizes of marital col are: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m df2\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaritalVector\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaritalSize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vector_size_udf' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType,StringType,DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def vector_size(x):\n",
    "    if isinstance(x, (DenseVector, SparseVector)):\n",
    "        return len(x.toArray())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df2 = df2.withColumn(\"maritalSize\",vector_size_udf(df2[\"maritalVector\"]))\n",
    "\n",
    "print(\"vectors sizes of marital col are: \")\n",
    "df2.select(\"maritalVector\", \"maritalSize\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "#3. For education feature\n",
    "#3. For education feature\n",
    "education_Index = StringIndexer(inputCol=\"education\", outputCol=\"educationIndex\")\n",
    "df3 = education_Index.fit(df2).transform(df2)\n",
    "print(\"vectors sizes of education col are: \")\n",
    "df3.show()\n",
    "\n",
    "education_vector = OneHotEncoder(inputCol=\"educationIndex\", outputCol=\"educationVector\")\n",
    "df4 = education_vector.fit(df3).transform(df3)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Vector assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\", \"maritalVector\",'educationVector'],\n",
    "    outputCol=\"vector\",\n",
    ")\n",
    "df5 = assembler.transform(df4)\n",
    "print(\"SHOW df5: \")\n",
    "df5.select(\"age\",\"maritalVector\",\"educationVector\",\"vector\").show(truncate=False)\n",
    "\n",
    "#5.\n",
    "pipeline = Pipeline(stages=[marital_Index,education_Index,marital_vector,education_vector,assembler])\n",
    "pipeline_model = pipeline.fit(df0)\n",
    "df5 = pipeline_model.transform(df0)\n",
    "\n",
    "df5.select(\"age\",\"marital\",\"education\",\"vector\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Y to label, using pipeline\n",
    "print(\"Step 6: \")\n",
    "label_indexer = StringIndexer(inputCol=\"y\", outputCol=\"label\")\n",
    "df6 = label_indexer.fit(df5).transform(df5)\n",
    "df6.select(\"y\",\"label\").show(truncate= False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Features col\n",
    "print(\"Step 7: \")\n",
    "numerical_features = [\n",
    "    \"age\",\n",
    "    \"duration\",\n",
    "    \"campaign\",\n",
    "    \"maritalVector\",\n",
    "    \"educationVector\"\n",
    "]\n",
    "assembler7 = VectorAssembler(inputCols= numerical_features,outputCol=\"features\")\n",
    "df_features = assembler7.transform(df6)\n",
    "df_features.select(\"age\",\"features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical Classification\n",
    "print(\"STATISTICAL CLASSIFICATION\")\n",
    "#1. Logistic regression -  add on appropriate hyperparams\n",
    "print(\"step 1: \")\n",
    "logistic_regression = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.8)\n",
    "\n",
    "pipeline = Pipeline(stages=[logistic_regression])\n",
    "pipeline_model = pipeline.fit(df_features)\n",
    "df_predictions = pipeline_model.transform(df_features)\n",
    "df_predictions.filter(df_predictions[\"prediction\"] == 0).select(\"label\",\"features\",\"prediction\").show(truncate=False)\n",
    "df_predictions.filter(df_predictions[\"prediction\"] == 1).select(\"label\",\"features\",\"prediction\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train and test the model\n",
    "pipeline_model.write().overwrite().save(\"/Users/hoangphuonganhnguyen/Desktop/intern/TP/[PRJ] Bank marketing/logistic_regression_pipeline\")\n",
    "print(\"Pipeline model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Apply the model to the dataset to generate predictions\n",
    "print(\"step 3: \")\n",
    "df_predictions = pipeline_model.transform(df_features)\n",
    "df_predictions.filter(df_predictions[\"prediction\"] == 0).select(\"label\",\"features\",\"prediction\").show(truncate=False)\n",
    "df_predictions.filter(df_predictions[\"prediction\"] == 1).select(\"label\",\"features\",\"prediction\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Extracting the Logistic regression (classifier) from the pipeline\n",
    "print(\"Pipeline model classifier: \")\n",
    "print(type(pipeline_model.stages[-1])) #<class 'pyspark.ml.classification.LogisticRegressionModel'>\n",
    "logistic_regression = LogisticRegression(featuresCol=\"features\",\n",
    "                                         labelCol=\"label\",\n",
    "                                         maxIter=10)\n",
    "logistic_model = logistic_regression.fit(df_features)\n",
    "\n",
    "#display the loss history\n",
    "loss_history = logistic_model.summary.objectiveHistory\n",
    "print(\"Loss History: \")\n",
    "print(loss_history)\n",
    "\n",
    "#Check convergence\n",
    "print(\"Check CONVERGENCE: \")\n",
    "if len(loss_history) > 1:\n",
    "    print(\"Loss changed between iterations are: \")\n",
    "    for i in range(1, len(loss_history)):\n",
    "        print(f\"Iteration {i}: Loss difference = {loss_history[i]-loss_history[i-1]}\")\n",
    "    else:\n",
    "        print(\"Not enough iterations to check convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance and Evaluation\n",
    "#1. Print AUC and assess the evaluation of of model classification\n",
    "print(\"Step 1: \")\n",
    "#Initialize the evaluator\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\",rawPredictionCol=\"rawPrediction\",metricName=\"areaUnderROC\")\n",
    "\n",
    "#Calculate the AUC\n",
    "auc = binary_evaluator.evaluate(df_predictions)\n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "#Visualize ROC\n",
    "roc = logistic_model.summary.roc.toPandas()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(roc['FPR'], roc['TPR'], label=\"ROC Curve\", color=\"blue\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\", label=\"Random Classifier\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. F-measures\n",
    "print(\"Retrieve F-Measure for different threshold: \")\n",
    "binary_summary = logistic_model.summary\n",
    "#extract data\n",
    "precision = binary_summary.precisionByThreshold\n",
    "recall = binary_summary.recallByThreshold\n",
    "f1_score = binary_summary.fMeasureByThreshold\n",
    "#convert to pandas df\n",
    "f1_df = f1_score.toPandas()\n",
    "precision_df = precision.toPandas()\n",
    "recall_df = recall.toPandas()\n",
    "\n",
    "print(\"F1 DataFrame:\")\n",
    "print(f1_df.head())\n",
    "print(\"Precision DataFrame:\")\n",
    "print(precision_df.head())\n",
    "print(\"Recall DataFrame:\")\n",
    "print(recall_df.head())\n",
    "\n",
    "#merge\n",
    "metrics_df = f1_df.merge(precision_df, on=\"threshold\").merge(recall_df, on=\"threshold\")\n",
    "metrics_df.columns = [\"Threshold\", \"F1-Score\", \"Precision\", \"Recall\"]\n",
    "\n",
    "#find the best threshold\n",
    "best_row = metrics_df.loc[metrics_df[\"F1-Score\"].idxmax()]\n",
    "best_threshold = best_row[\"Threshold\"]\n",
    "best_f1 = best_row[\"F1-Score\"]\n",
    "\n",
    "print(f\"Best F1-Score: {best_f1}\")\n",
    "print(f\"Best threshold: {best_threshold}\")\n",
    "\n",
    "#apply threshold to the model\n",
    "logistic_model.setThreshold(best_threshold)\n",
    "print(f\"Best classifier threshold set to: {best_threshold}\")\n",
    "logistic_model.write().overwrite().save(\"optimized_logistic_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. PLot the F-measure evaluation\n",
    "print(\"F-Measure PLot: \")\n",
    "thresholds = metrics_df[\"Threshold\"]\n",
    "f1_scores = metrics_df[\"F1-Score\"]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(thresholds, f1_scores, label=\"F1-Score\", color=\"blue\", linewidth=2)\n",
    "plt.xlabel(\"Threshold\", fontsize=12)\n",
    "plt.ylabel(\"F1-Score\", fontsize=12)\n",
    "plt.title(\"F1-Score vs Threshold\", fontsize=14)\n",
    "plt.axvline(x=best_threshold, color='red', linestyle='--', label=f\"Best Threshold: {best_threshold:.4f}\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4, Train model on 5-folds approach\n",
    "print(\"Step 4: \")\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(logistic_model.regParam, [0.1, 0.3, 0.5]) \\\n",
    "        .addGrid(logistic_model.elasticNetParam, [0.0,0.5,1.0])\\\n",
    "            .build()\n",
    "\n",
    "#set up binary classification\n",
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\", rawPredictionCol=\"rawPrediction\")\n",
    "CrossValidator = CrossValidator(estimator=logistic_regression,\n",
    "                                estimatorParamMaps=paramGrid,\n",
    "                                evaluator=evaluator,\n",
    "                                numFolds= 5)\n",
    "#Perform cross-validation\n",
    "cvModel = CrossValidator.fit(df_features)\n",
    "\n",
    "#Get and evaluate the best model\n",
    "bestModel = cvModel.bestModel\n",
    "auc = evaluator.evaluate(bestModel.transform(df_features))\n",
    "print(f\"Best model after cross-validation is: {auc}\")\n",
    "\n",
    "#Best model after cross-validation is: 0.8375070526053597\n",
    "#the difference between the number from step 1 and the number from step 4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
